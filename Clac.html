# CRR_project_app/llm_services.py
import requests

class CRRAgent:
    def ask_ollama(self, prompt: str) -> str:
        try:
            response = requests.post(
                'http://localhost:11434/api/chat',
                json={
                    "model": "llama2",
                    "messages": [{"role": "user", "content": prompt}]
                },
                timeout=30  # Increase timeout to 30 seconds
            )
            response.raise_for_status()  # Raise HTTP errors
            return response.json()['message']['content']
        except requests.exceptions.RequestException as e:
            return f"Error: Could not connect to Ollama. Details: {str(e)}"
        except KeyError:
            return "Error: Invalid response format from Ollama."
